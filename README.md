# HA-First Cluster Platform (from scratch)

## ğŸ¯ Objectif du projet
Ce projet vise Ã  construire from scratch une plateforme de type cluster HA, oÃ¹ la continuitÃ© de service est une propriÃ©tÃ© fondamentale du systÃ¨me.

Lâ€™objectif est de permettre :

- lâ€™exÃ©cution de workloads sur plusieurs machines,
- la reprise automatique en cas de panne,
- la conservation de lâ€™Ã©tat applicatif,
- une coupure client nulle ou minimale.

La plateforme est pensÃ©e comme un socle gÃ©nÃ©rique, indÃ©pendant dâ€™une application particuliÃ¨re.

## ğŸ§  Positionnement technique (choix assumÃ©s)
### Approche

- ImplÃ©mentation from scratch (pas Kubernetes, pas Swarm).
- Architecture inspirÃ©e des systÃ¨mes distribuÃ©s modernes (orchestrateurs, schedulers, control plane).
- PrioritÃ© Ã  la comprÃ©hension, la maÃ®trise et lâ€™Ã©volutivitÃ©.

### Choix volontairement simplifiÃ© (Phase I)

- Une base de donnÃ©es centrale
- Accessible via un seul point logique
- Non redondÃ©e pour lâ€™instant

ğŸ‘‰ Ce choix est conscient et temporaire, afin de :

- rÃ©duire la complexitÃ© initiale,
- accÃ©lÃ©rer le dÃ©veloppement du cÅ“ur du cluster,
- se concentrer sur le HA des workloads, pas encore sur le HA des donnÃ©es.

## ğŸ—ï¸ Architecture globale
### 1. Control Plane (HA)

Le cluster repose sur un plan de contrÃ´le chargÃ© de :

- maintenir lâ€™Ã©tat global du cluster,
- connaÃ®tre les nÅ“uds disponibles,
- dÃ©cider oÃ¹ lancer les workloads,
- dÃ©tecter les pannes.

CaractÃ©ristiques :

- plusieurs instances possibles
- consensus / quorum prÃ©vu
- aucune instance unique critique Ã  terme

### 2. Workers

Les workers sont des machines dâ€™exÃ©cution :

- ils reÃ§oivent des ordres du control plane,
- exÃ©cutent les workloads (conteneurs/process),
- remontent leur Ã©tat (heartbeat, santÃ©, charge).

Un worker peut tomber sans interrompre le service global.

### 3. Base de donnÃ©es (Phase I)

La base de donnÃ©es est utilisÃ©e pour :

-  lâ€™Ã©tat du cluster (mÃ©tadonnÃ©es),
- lâ€™Ã©tat applicatif (config, sessions, jobs, etc.).

CaractÃ©ristiques actuelles :

- un seul endpoint
- pas de rÃ©plication
- SPOF assumÃ©

Limitation connue :

```
Si la base tombe, le cluster ne peut plus Ã©voluer,
mais les workloads dÃ©jÃ  lancÃ©s peuvent continuer Ã  tourner.
```
La redondance de la base est explicitement reportÃ©e Ã  une phase ultÃ©rieure.

### ğŸ” Gestion du cycle de vie des workloads

Le systÃ¨me fonctionne par intention :

- lâ€™utilisateur dÃ©finit un desired state
- le cluster sâ€™assure que lâ€™Ã©tat rÃ©el converge vers cet objectif
- toute divergence (panne, crash, perte de nÅ“ud) dÃ©clenche une correction automatique

### ğŸŒ Routage et continuitÃ© client

- Les clients se connectent Ã  un endpoint stable
- Le routage interne est dynamique
- Les instances dÃ©faillantes sont retirÃ©es automatiquement

Objectif :

- aucune configuration client Ã  modifier
- reconnexion Ã©ventuelle, mais rapide et transparente

### ğŸ“¦ Gestion de lâ€™Ã©tat applicatif

Principe clÃ© :

- Aucun Ã©tat critique ne doit Ãªtre stockÃ© localement sur un worker

Phase I :

- Ã©tat centralisÃ© en base unique
- accÃ¨s contrÃ´lÃ© par le cluster

Phase II (future) :

- rÃ©plication
- leader election
- bascule automatique
- suppression du SPOF base de donnÃ©es

### ğŸš§ Limites actuelles (connues et acceptÃ©es)

- La base de donnÃ©es est un point unique de dÃ©faillance
- Le projet ne vise pas encore :
  - le multi-DC
  - la tolÃ©rance totale aux partitions rÃ©seau
- Lâ€™objectif est la stabilitÃ© fonctionnelle, pas la perfection thÃ©orique

## ğŸ›£ï¸ Roadmap simplifiÃ©e
### Phase I â€” Fondation

- cluster from scratch
- control plane fonctionnel
- workers + scheduling
- base centrale unique
- HA des workloads

### Phase II â€” Robustesse

- rÃ©plication de la base
- leader election DB
- tolÃ©rance aux pannes de donnÃ©es
- rÃ©duction drastique du SPOF

### Phase III â€” MaturitÃ©

- rolling updates
- autoscaling
- observabilitÃ© avancÃ©e
- politiques HA par dÃ©faut

### ğŸ§ª CritÃ¨re de rÃ©ussite Phase I

Le projet est considÃ©rÃ© valide si :

- un service tourne sur plusieurs workers
- un worker est coupÃ© brutalement
- le service est relancÃ© ailleurs automatiquement
- le client continue Ã  accÃ©der au service
- lâ€™Ã©tat applicatif est conservÃ© (tant que la DB est disponible)

### ğŸ§© Vision

Ce projet nâ€™essaie pas de battre les solutions existantes.
Il vise Ã  comprendre, maÃ®triser et reconstruire les fondations dâ€™un systÃ¨me HA moderne.

```
La haute disponibilitÃ© nâ€™est pas un add-on.
Câ€™est une propriÃ©tÃ© structurelle du systÃ¨me.
```